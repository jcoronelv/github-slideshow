{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Introducción al aprendizaje computacional\n",
    "\n",
    "## Actividad 1\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "\n",
    "\n",
    "En esta actividad vamos a estudiar una primera aproximación a los modelos de aprendizaje computacional, utilizando como base un problema de clasifiación y el modelo de clasificación logístico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad en grupos\n",
    "### Nombres:\n",
    "\n",
    "- Juan Camilo Marin Cala \n",
    "- Juan David Cortes Castillo\n",
    "- Jonny Eduardo Coronel Villamil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aprendizaje computacional\n",
    "\n",
    "El aprendizaje computacional o **Machine Learning** (ML), utiliza algoritmos con parámetros libres que se puedan ajustar de manera automática, con el objetivo de mejorar el desempeño de los modelos a partir de la información disponible.\n",
    "\n",
    "El aprendizaje computacional se circunscribe en el campo de la Inteligencia Computacional, o lo que se conoce como Inteligencia Artificial, mediante la búsqueda de patrones a partir de los *Datos*. Entonces, los algoritmos desarrollados dentro del Machine Learning (ML a partir de ahora) se pueden entender como los bloques fundacionales que permiten aprender computacionalmente a partir de los datos. De esta manera, generalizando los datos en lugar de solo almacenarlos y devolver busquedas específicas, como en los sistemas relacionales de bases de datos.\n",
    "\n",
    "\n",
    "### Tipos de aprendizaje\n",
    "\n",
    "Los tipos de problemas sobre los que se trabaja en ML se pueden entender como de tipo **supervisado**, **no supervisado**, y **semi-supervisado**. \n",
    "\n",
    "**Aprendizaje supervisado**\n",
    "\n",
    "El aprendizaje supervisado se refiere a modelos, o conjuntos de algoritmos, que aproximan o estiman una función $f(x)$ que representa la relación entre la variable dependiente $Y$ (etiqueta o valor objetivo) y el conjunto de variables independientes $X$. Por ello a los algoritmos de tipo supervisado se les asocia usualmente con modelos predictivos, donde dado un conjunto de datos $X$, se puede predecir un nuevo valor de la variable dependiente $Y$. \n",
    "\n",
    "De acuerdo con el tipo de valores en $Y$, se pueden definir dos tipos principales de problemas y algoritmos para analizar los datos:\n",
    "\n",
    "- Problemas de clasificación\n",
    "\n",
    "Siempre que la variable $Y$ se refiera a un grupo de categorías (valores sin ningún orden en particular), como por ejemplo bueno/malo, pequeño/grande, la tarea de predecir $Y$ puede ser considerada como un problema de clasificación. En este sentido, las variables de salida se conocen como etiquetas o categorías.\n",
    "\n",
    "- Problemas de regresión\n",
    "\n",
    "Un problema de regresión consiste en estimar y/o predecir una variable dependiente (o valor objetivo) $Y$ con valores continuos. Por ejemplo, predecir el precio de una vivienda, de acciones, alimentos, etc. \n",
    "\n",
    "**Aprendizaje no-supervisado**\n",
    "    \n",
    "El aprendizaje no-supervisado considera problemas donde la variable dependiente $Y$, o las etiquetas para el conjunto de datos, no está disponible. Es decir, cuando $Y$ no está contenida en el conjunto de datos. Entonces, en lugar de estimar o predecir una variable, un algoritmo no-supervisado utiliza técnicas sobre el conjunto de datos de entrada $X$ para detectar patrones, encontrar reglas, o resumir y agrupar los datos. Usualmente, los algoritmos no-supervisados son utilizados para el análisis descriptivo y la modelación, donde se necesita una primera aproximación a los datos, desarrollar una intuición y extraer nuevo conocmiento que es desconocido para el analista y/o experto. \n",
    "\n",
    "En el aprendizaje no-supervisado, se tienen dos tareas principales, la reducción de dimensionalidad y el análisis de clustering.\n",
    "\n",
    "- Reducción de dimensionalidad \n",
    "\n",
    "La reducción de dimensionalidad busca encontrar la estructura subyacente de los datos, reduciendo la cantidad de información disponible en el conjunto de datos $X$. Por ejemlo, es muy conocido el análisis de componentes principales. \n",
    "\n",
    "- Clustering \n",
    "\n",
    "El análisis de clustering consiste en agrupar un conjunto de datos $X$ de manera que cada grupo contenga observaciones más similares entre sí que con las observaciones de otros grupos. Estos grupos son denominados *clusters*. Es una técnica bastante común para la exploración de los datos y su análisis. \n",
    "\n",
    "\n",
    "#### Otros tipos de aprendizaje\n",
    "\n",
    "Más allá de estos dos tipos de aprendizaje presentados anteriormente, existen otros tipos que son bastante útiles dependiendo de la naturaleza del problema. Por ejemplo, el **aprendizaje semi-supervisado** (https://medium.com/inside-machine-learning/placeholder-3557ebb3d470) o el **aprendizaje por refuerzo** (https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problema de clasificación: riesgo de default\n",
    "\n",
    "### 2.1 Datos\n",
    "\n",
    "Como hemos visto, una tarea muy usual dentro del ML es la de la clasificación. Pero antes, vamos a importar las bibliotecas que vamos a usar en este cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos paquetes iniciales que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"germancredit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Default</th>\n",
       "      <th>checkingstatus1</th>\n",
       "      <th>duration</th>\n",
       "      <th>history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employ</th>\n",
       "      <th>installment</th>\n",
       "      <th>status</th>\n",
       "      <th>...</th>\n",
       "      <th>residence</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>otherplans</th>\n",
       "      <th>housing</th>\n",
       "      <th>cards</th>\n",
       "      <th>job</th>\n",
       "      <th>liable</th>\n",
       "      <th>tele</th>\n",
       "      <th>foreign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A11</td>\n",
       "      <td>6</td>\n",
       "      <td>A34</td>\n",
       "      <td>A43</td>\n",
       "      <td>1169</td>\n",
       "      <td>A65</td>\n",
       "      <td>A75</td>\n",
       "      <td>4</td>\n",
       "      <td>A93</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>A121</td>\n",
       "      <td>67</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A192</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A12</td>\n",
       "      <td>48</td>\n",
       "      <td>A32</td>\n",
       "      <td>A43</td>\n",
       "      <td>5951</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>2</td>\n",
       "      <td>A92</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>A121</td>\n",
       "      <td>22</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>1</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A14</td>\n",
       "      <td>12</td>\n",
       "      <td>A34</td>\n",
       "      <td>A46</td>\n",
       "      <td>2096</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>A121</td>\n",
       "      <td>49</td>\n",
       "      <td>A143</td>\n",
       "      <td>A152</td>\n",
       "      <td>1</td>\n",
       "      <td>A172</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A11</td>\n",
       "      <td>42</td>\n",
       "      <td>A32</td>\n",
       "      <td>A42</td>\n",
       "      <td>7882</td>\n",
       "      <td>A61</td>\n",
       "      <td>A74</td>\n",
       "      <td>2</td>\n",
       "      <td>A93</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>A122</td>\n",
       "      <td>45</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>1</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A11</td>\n",
       "      <td>24</td>\n",
       "      <td>A33</td>\n",
       "      <td>A40</td>\n",
       "      <td>4870</td>\n",
       "      <td>A61</td>\n",
       "      <td>A73</td>\n",
       "      <td>3</td>\n",
       "      <td>A93</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>A124</td>\n",
       "      <td>53</td>\n",
       "      <td>A143</td>\n",
       "      <td>A153</td>\n",
       "      <td>2</td>\n",
       "      <td>A173</td>\n",
       "      <td>2</td>\n",
       "      <td>A191</td>\n",
       "      <td>A201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Default checkingstatus1  duration history purpose  amount savings employ  \\\n",
       "0        0             A11         6     A34     A43    1169     A65    A75   \n",
       "1        1             A12        48     A32     A43    5951     A61    A73   \n",
       "2        0             A14        12     A34     A46    2096     A61    A74   \n",
       "3        0             A11        42     A32     A42    7882     A61    A74   \n",
       "4        1             A11        24     A33     A40    4870     A61    A73   \n",
       "\n",
       "   installment status  ... residence  property age  otherplans housing cards  \\\n",
       "0            4    A93  ...         4      A121  67        A143    A152     2   \n",
       "1            2    A92  ...         2      A121  22        A143    A152     1   \n",
       "2            2    A93  ...         3      A121  49        A143    A152     1   \n",
       "3            2    A93  ...         4      A122  45        A143    A153     1   \n",
       "4            3    A93  ...         4      A124  53        A143    A153     2   \n",
       "\n",
       "    job liable  tele foreign  \n",
       "0  A173      1  A192    A201  \n",
       "1  A173      1  A191    A201  \n",
       "2  A172      2  A191    A201  \n",
       "3  A173      2  A191    A201  \n",
       "4  A173      2  A191    A201  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Default             int64\n",
       "checkingstatus1    object\n",
       "duration            int64\n",
       "history            object\n",
       "purpose            object\n",
       "amount              int64\n",
       "savings            object\n",
       "employ             object\n",
       "installment         int64\n",
       "status             object\n",
       "others             object\n",
       "residence           int64\n",
       "property           object\n",
       "age                 int64\n",
       "otherplans         object\n",
       "housing            object\n",
       "cards               int64\n",
       "job                object\n",
       "liable              int64\n",
       "tele               object\n",
       "foreign            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    700\n",
       "1    300\n",
       "Name: Default, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Default.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase positiva (default=1) tiene menos casos.  Además, es 5 veces más costoso clasificar a un usuario como bueno (Defualt=0) cuando es malo (Default=1), que clasificarlo como malo cuando es bueno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A201    963\n",
       "A202     37\n",
       "Name: foreign, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.foreign.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la mayoría de clientes de la base de datos son extranjeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Codificacion de variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Default  duration  amount  installment  residence  age  cards  liable  \\\n",
      "0        0         6    1169            4          4   67      2       1   \n",
      "1        1        48    5951            2          2   22      1       1   \n",
      "2        0        12    2096            2          3   49      1       2   \n",
      "3        0        42    7882            2          4   45      1       2   \n",
      "4        1        24    4870            3          4   53      2       2   \n",
      "\n",
      "   checkingstatus1_A11  checkingstatus1_A12  ...  housing_A152  housing_A153  \\\n",
      "0                    1                    0  ...             1             0   \n",
      "1                    0                    1  ...             1             0   \n",
      "2                    0                    0  ...             1             0   \n",
      "3                    1                    0  ...             0             1   \n",
      "4                    1                    0  ...             0             1   \n",
      "\n",
      "   job_A171  job_A172  job_A173  job_A174  tele_A191  tele_A192  foreign_A201  \\\n",
      "0         0         0         1         0          0          1             1   \n",
      "1         0         0         1         0          1          0             1   \n",
      "2         0         1         0         0          1          0             1   \n",
      "3         0         0         1         0          1          0             1   \n",
      "4         0         0         1         0          1          0             1   \n",
      "\n",
      "   foreign_A202  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "credit_1 = data.copy()\n",
    "credit_1 = pd.get_dummies(credit_1, columns=['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'], prefix = ['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'])\n",
    "\n",
    "print(credit_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 61)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = credit_1.iloc[:, 1:62]#[['duration','amount','installment','residence','age','cards','liable']]\n",
    "Y = credit_1.iloc[:, 0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regresión logística\n",
    "\n",
    "Podemos estudiar un problema de clasificación desde una perspectiva probabilística, examinando una situación donde la variable respuesta ($Y_i$) consiste de dos categorías ($\\{0,1\\}$). La *regresión logística* estima la probabilidad de que una observación pertenezca a una de las dos categorías.\n",
    "\n",
    "Entonces se modela la función de probabilidad $p(Y_i=1)$ de tal manera que tome un valor entre 0 y 1. La función de regresión logística consiste en la función *sigmoide* ($\\sigma(\\cdot)$) $$ p(Y_i)=p(Y_i=1)=\\frac{e^{\\beta_0 + \\beta_1X_{1} + ... + \\beta_k X_{k}}}{1+e^{\\beta_0 + \\beta_1X_{1} + ... + \\beta_k X_{k}}}=\\frac{e^{Z_i}}{1+e^{Z_i}}=\\sigma(Z_i)$$\n",
    "donde se tiene que \n",
    "$$log\\frac{p(Y_i)}{1-p(Y_i)}=\\beta_0 + \\beta_1X_{1} + ... + \\beta_k X_{k}.  $$\n",
    "\n",
    "\n",
    "Por lo tanto, manteniendo todo lo demás constante, se puede estimar el cambio que una unidad extra en $X_1$ tiene sobre  el chance (o el *log-odds*) $log\\frac{p(Y_i)}{1-p(Y_i)}$. Esta estimación está dada por $\\hat \\beta_1$. \n",
    "\n",
    "\n",
    "Los coeficientes del modelo se pueden estimar por el método de máxima verosimilitud, buscando iterativamente estimadores que maximicen la función de verosimilitud: $$ F_{\\mathbf{\\beta}}=\\prod_{i:Y_i=1} p(Y_i)\\prod_{i':Y_{i'}=0}(1-p(Y_{i'})). $$\n",
    "\n",
    "\n",
    "Finalmente, la extensión del modelo logístico para múltiples clases o categorías ($C$) se hace calculando la probabilidad de una categoría ($C_i$) frente al resto y utilizando lo que se conoce como una función *softmax*: \n",
    "$$ \\sigma (Z_i )={\\frac {e^{Z_{i}}}{\\sum _{j=1}^{|C|}e^{Z_{j}}}}{\\text{ para }}i=1,\\dotsc ,|C|$$\n",
    "\n",
    "\n",
    "Miremos un ejemplo con nuestros datos sobre el comportamiento de los clientes del banco. Primero descargamos los paquetes que vamos a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, la funcion `LogisticRegression()` encuentra una solución mediante el algoritmo de Broyden–Fletcher–Goldfarb–Shanno (BFGS):  https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.47469507e-02  1.09810047e-04  3.13259433e-01  1.97259480e-02\n",
      "  -1.90902305e-02  7.35192452e-02 -5.01660732e-02  6.38315796e-01\n",
      "   2.34271038e-01 -2.38487630e-01 -1.07331615e+00  4.21198205e-01\n",
      "   3.54752521e-01 -1.57059639e-01 -8.90074593e-02 -9.69100572e-01\n",
      "   6.91485785e-01 -9.04866714e-01 -1.31006755e-01 -2.85688223e-01\n",
      "  -3.21845796e-01  2.27950271e-02  1.69726850e-01  4.07911630e-01\n",
      "  -1.07196152e-01  1.94674030e-02  3.25535091e-01  1.52249659e-01\n",
      "  -5.20776530e-02 -3.38558459e-01 -5.26365583e-01  3.61924656e-02\n",
      "   1.20318352e-01  5.80559707e-02 -6.70107191e-01  1.63234578e-02\n",
      "   2.42877428e-01 -8.02687408e-03 -5.10142314e-01 -1.63925185e-01\n",
      "  -2.54427798e-02  1.79099001e-01 -5.92873166e-01 -4.44178441e-01\n",
      "  -1.45334199e-01 -3.34598939e-02  1.83755590e-01  4.88112466e-02\n",
      "   1.13739155e-01 -6.01767347e-01  2.96180724e-02 -3.40764843e-01\n",
      "  -1.28070175e-01 -8.74712447e-02 -1.37606326e-01 -4.63003634e-02\n",
      "  -1.67839011e-01 -1.35574493e-01 -3.03642452e-01  5.57356664e-02\n",
      "  -4.94952611e-01]]\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression(penalty='none', max_iter= 1500) \n",
    "log.fit(X, Y)\n",
    "y_pred_log = log.predict(X)\n",
    "log_coef = log.coef_\n",
    "print(log_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([368])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3.1\n",
    "\n",
    "Ajuste las opciones de la funcion `LogisticRegression`para conseguir que la solucion converja. Puede examinar la documentación de Python: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corre el modelo con un maximo de 1500 iteraciones, con la expectativa que este converja  en un número menor de iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1500, penalty='none')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log = LogisticRegression(penalty='none', max_iter= 1500 ,  ) \n",
    "log.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 3.2 \n",
    "\n",
    "Cuántas iteraciones necesitó?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo requirio 368 iteraciones para que la regresión logistica convergiera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([368])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construccion de modelos predictivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos los datos de manera aleatoria en un conjunto de entrenamiento y otro de prueba. De esta manera, estimamos los coeficientes sobre los datos de entrenamiento, y ese mismo modelo lo probamos sobre los datos de prueba con el fin de controlar que el modelo esté generalizando bien y no se sobreajuste a los datos.\n",
    "\n",
    "A continuación ejecutamos el codigo para obtener una partición con el 40% de los datos en el conjunto de prueba. Nótese que por defecto la funcion `train_test_split` sigue una partición estratificada, es decir, mantiene la distribución inicial de las clases en ambos conjuntos de entrenamiento y prueba:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de CE, CP:  (600,) (400,)\n",
      "Observaciones de la clase positiva en entrenamiento: 182 y en prueba: 118\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "print(\"Tamaño de CE, CP: \", y_train.shape, y_test.shape)\n",
    "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(y_train)) +\" y en prueba: \" +str(sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo podemos verificar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de CE, CP:  (600,) (400,)\n",
      "Observaciones de la clase positiva en entrenamiento: 180 y en prueba: 120\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "print(\"Tamaño de CE, CP: \", y_train.shape, y_test.shape)\n",
    "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(y_train)) +\" y en prueba: \" +str(sum(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ajustamos el modelo logístico y lo probamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logT = LogisticRegression(penalty='none', max_iter=1500)\n",
    "logT.fit(X_train, y_train)\n",
    "y_tr = logT.predict(X_train)\n",
    "y_pred = logT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con menos datos para entrenar probablemente el algoritmo necesite más iteraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([749])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logT.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos los coeficientes del modelo y su desviación con respecto a la estimación anterior (que utilizaba todos los datos de la muestra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.52686652e-02  1.02117165e-04  3.48238974e-01  2.55262100e-02\n",
      "  -9.18675274e-03  1.98510037e-01  3.31242815e-01  5.08645527e-01\n",
      "   1.70840957e-01 -3.81353187e-01 -1.08364275e+00  3.24147953e-01\n",
      "   3.82934375e-01 -1.65706225e-01 -1.74147875e-01 -1.15273769e+00\n",
      "   1.73371035e-01 -9.70392361e-01 -5.32480916e-01 -2.62442743e-01\n",
      "  -2.93405390e-01 -9.55614662e-02  1.56124717e-01  1.05501583e+00\n",
      "  -1.61675762e-03 -1.41214041e-02  7.20259235e-01  2.52254964e-01\n",
      "  -2.81078704e-01 -1.12978346e+00 -3.47161490e-01  7.19697209e-01\n",
      "   3.65770120e-02 -2.36368886e-01 -1.14934528e+00 -1.56069512e-01\n",
      "   2.80181724e-01 -2.03988297e-01 -6.22485242e-01 -2.39217642e-01\n",
      "  -2.24723368e-01  2.89080095e-01 -8.49866184e-01 -5.19869712e-01\n",
      "  -1.89204161e-01 -2.56732374e-01  1.80296790e-01  5.25383654e-02\n",
      "  -2.31091458e-02 -8.14938677e-01  4.99417404e-01 -4.22439082e-01\n",
      "  -8.62487778e-01 -1.37570495e-01 -1.26214792e-01 -1.55628938e-01\n",
      "  -3.66095231e-01 -2.10119295e-01 -5.75390162e-01  4.37517535e-01\n",
      "  -1.22302699e+00]]\n"
     ]
    }
   ],
   "source": [
    "logT_coef = logT.coef_\n",
    "print(logT_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.21714493e-04  7.69288175e-06 -3.49795413e-02 -5.80026201e-03\n",
      "  -9.90347773e-03 -1.24990791e-01 -3.81408888e-01  1.29670269e-01\n",
      "   6.34300801e-02  1.42865557e-01  1.03266050e-02  9.70502517e-02\n",
      "  -2.81818539e-02  8.64658554e-03  8.51404161e-02  1.83637113e-01\n",
      "   5.18114750e-01  6.55256472e-02  4.01474161e-01 -2.32454805e-02\n",
      "  -2.84404059e-02  1.18356493e-01  1.36021323e-02 -6.47104198e-01\n",
      "  -1.05579394e-01  3.35888071e-02 -3.94724144e-01 -1.00005306e-01\n",
      "   2.29001051e-01  7.91225004e-01 -1.79204093e-01 -6.83504743e-01\n",
      "   8.37413399e-02  2.94424857e-01  4.79238088e-01  1.72392970e-01\n",
      "  -3.73042960e-02  1.95961423e-01  1.12342928e-01  7.52924575e-02\n",
      "   1.99280588e-01 -1.09981094e-01  2.56993018e-01  7.56912706e-02\n",
      "   4.38699620e-02  2.23272480e-01  3.45879938e-03 -3.72711886e-03\n",
      "   1.36848301e-01  2.13171329e-01 -4.69799331e-01  8.16742397e-02\n",
      "   7.34417604e-01  5.00992506e-02 -1.13915335e-02  1.09328575e-01\n",
      "   1.98256220e-01  7.45448020e-02  2.71747710e-01 -3.81781868e-01\n",
      "   7.28074380e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(log_coef-logT_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4.1\n",
    "\n",
    "Implemente un proceso de validación cruzada cambiando la semilla de las particiones de train (CE) y test (CP). Estimae la varianza de los estimadores y concluya si su *mejor* modelo es estable entre distintas particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_logistico  =  LogisticRegression(penalty='none', max_iter=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(modelo_logistico, X, Y, cv=20,return_train_score=True , return_estimator =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramatros = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "Se estiman 100 modelos de regresiòn logistica cada uno con una semilla diferente en la partición y guardamos los parametros de cada uno de ellos para determinar su varianza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(0,100): \n",
    "    modelo_logistico  =  LogisticRegression(penalty='none', max_iter=1500)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.4, random_state=i )   \n",
    "    modelo_logistico.fit(X_train,y_train )\n",
    "    ax = pd.DataFrame(modelo_logistico.coef_)\n",
    "    paramatros = paramatros.append(ax, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se observa para cada iteración se almacenan los parametros para posteriormente calcular sus varianzas y estabilidad en los parametros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.032671</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.256003</td>\n",
       "      <td>-0.058306</td>\n",
       "      <td>-0.003321</td>\n",
       "      <td>0.286603</td>\n",
       "      <td>0.066050</td>\n",
       "      <td>0.874769</td>\n",
       "      <td>0.078894</td>\n",
       "      <td>-0.731736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227157</td>\n",
       "      <td>-0.897767</td>\n",
       "      <td>-0.610010</td>\n",
       "      <td>-0.083917</td>\n",
       "      <td>-0.366080</td>\n",
       "      <td>0.228062</td>\n",
       "      <td>-0.096377</td>\n",
       "      <td>-0.735567</td>\n",
       "      <td>0.414970</td>\n",
       "      <td>-1.246915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025095</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.445282</td>\n",
       "      <td>0.104586</td>\n",
       "      <td>-0.030611</td>\n",
       "      <td>0.590142</td>\n",
       "      <td>-0.093780</td>\n",
       "      <td>0.517398</td>\n",
       "      <td>-0.019519</td>\n",
       "      <td>-0.144523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361042</td>\n",
       "      <td>-0.463306</td>\n",
       "      <td>-0.386534</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>-0.244736</td>\n",
       "      <td>-0.085657</td>\n",
       "      <td>-0.129780</td>\n",
       "      <td>-0.583002</td>\n",
       "      <td>-0.105507</td>\n",
       "      <td>-0.607274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021471</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.351746</td>\n",
       "      <td>0.082672</td>\n",
       "      <td>-0.016930</td>\n",
       "      <td>-0.146167</td>\n",
       "      <td>0.722969</td>\n",
       "      <td>0.647529</td>\n",
       "      <td>0.408096</td>\n",
       "      <td>-0.889756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.412127</td>\n",
       "      <td>-0.467079</td>\n",
       "      <td>-0.627033</td>\n",
       "      <td>-0.001143</td>\n",
       "      <td>-0.158485</td>\n",
       "      <td>-0.007982</td>\n",
       "      <td>-0.201590</td>\n",
       "      <td>-0.593053</td>\n",
       "      <td>0.148284</td>\n",
       "      <td>-0.942927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050314</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.185378</td>\n",
       "      <td>0.026086</td>\n",
       "      <td>-0.037256</td>\n",
       "      <td>0.187264</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>0.610711</td>\n",
       "      <td>0.105193</td>\n",
       "      <td>-0.177144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057431</td>\n",
       "      <td>-0.571172</td>\n",
       "      <td>-0.821983</td>\n",
       "      <td>0.241457</td>\n",
       "      <td>0.091958</td>\n",
       "      <td>-0.080729</td>\n",
       "      <td>-0.111169</td>\n",
       "      <td>-0.458128</td>\n",
       "      <td>0.345634</td>\n",
       "      <td>-0.914931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024697</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.202105</td>\n",
       "      <td>-0.122979</td>\n",
       "      <td>-0.005364</td>\n",
       "      <td>0.411503</td>\n",
       "      <td>-0.060393</td>\n",
       "      <td>0.675709</td>\n",
       "      <td>0.136873</td>\n",
       "      <td>-0.132811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156505</td>\n",
       "      <td>-0.310036</td>\n",
       "      <td>-0.143884</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>-0.094179</td>\n",
       "      <td>-0.230241</td>\n",
       "      <td>-0.154570</td>\n",
       "      <td>-0.312192</td>\n",
       "      <td>0.163843</td>\n",
       "      <td>-0.630605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031245</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.410536</td>\n",
       "      <td>0.069726</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.462986</td>\n",
       "      <td>0.522081</td>\n",
       "      <td>0.745727</td>\n",
       "      <td>0.200726</td>\n",
       "      <td>-0.834184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.481257</td>\n",
       "      <td>-0.514269</td>\n",
       "      <td>-0.263155</td>\n",
       "      <td>-0.068381</td>\n",
       "      <td>-0.377578</td>\n",
       "      <td>-0.311870</td>\n",
       "      <td>-0.351235</td>\n",
       "      <td>-0.669750</td>\n",
       "      <td>-0.077694</td>\n",
       "      <td>-0.943291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.035841</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.184033</td>\n",
       "      <td>-0.014322</td>\n",
       "      <td>-0.009471</td>\n",
       "      <td>0.088641</td>\n",
       "      <td>-0.151515</td>\n",
       "      <td>0.774915</td>\n",
       "      <td>0.218633</td>\n",
       "      <td>-0.071132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287983</td>\n",
       "      <td>-0.190941</td>\n",
       "      <td>-0.106361</td>\n",
       "      <td>-0.117559</td>\n",
       "      <td>-0.040216</td>\n",
       "      <td>-0.177651</td>\n",
       "      <td>-0.114126</td>\n",
       "      <td>-0.327663</td>\n",
       "      <td>-0.160293</td>\n",
       "      <td>-0.281495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.026395</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.353871</td>\n",
       "      <td>-0.092337</td>\n",
       "      <td>-0.017035</td>\n",
       "      <td>-0.064316</td>\n",
       "      <td>0.182909</td>\n",
       "      <td>0.706089</td>\n",
       "      <td>0.135378</td>\n",
       "      <td>-0.309628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445184</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.180830</td>\n",
       "      <td>0.089259</td>\n",
       "      <td>0.036199</td>\n",
       "      <td>-0.372982</td>\n",
       "      <td>-0.186342</td>\n",
       "      <td>-0.242012</td>\n",
       "      <td>0.055065</td>\n",
       "      <td>-0.483419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.015057</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.300439</td>\n",
       "      <td>-0.002715</td>\n",
       "      <td>-0.019450</td>\n",
       "      <td>-0.251944</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.550600</td>\n",
       "      <td>0.154253</td>\n",
       "      <td>-0.095307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364931</td>\n",
       "      <td>0.046038</td>\n",
       "      <td>-0.219722</td>\n",
       "      <td>0.310221</td>\n",
       "      <td>-0.055993</td>\n",
       "      <td>-0.431610</td>\n",
       "      <td>-0.071361</td>\n",
       "      <td>-0.325745</td>\n",
       "      <td>0.096347</td>\n",
       "      <td>-0.493453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.377367</td>\n",
       "      <td>-0.175569</td>\n",
       "      <td>-0.015515</td>\n",
       "      <td>0.477543</td>\n",
       "      <td>0.204756</td>\n",
       "      <td>0.697222</td>\n",
       "      <td>0.250206</td>\n",
       "      <td>-0.785094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533982</td>\n",
       "      <td>-0.546195</td>\n",
       "      <td>-0.168957</td>\n",
       "      <td>0.063893</td>\n",
       "      <td>-0.169763</td>\n",
       "      <td>-0.570421</td>\n",
       "      <td>-0.325795</td>\n",
       "      <td>-0.519452</td>\n",
       "      <td>0.587066</td>\n",
       "      <td>-1.432313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.036167</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.327625</td>\n",
       "      <td>-0.092358</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.498002</td>\n",
       "      <td>0.084638</td>\n",
       "      <td>0.632592</td>\n",
       "      <td>0.082388</td>\n",
       "      <td>-0.287607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.410985</td>\n",
       "      <td>-0.582586</td>\n",
       "      <td>0.255945</td>\n",
       "      <td>-0.128399</td>\n",
       "      <td>-0.345089</td>\n",
       "      <td>-0.527155</td>\n",
       "      <td>-0.275819</td>\n",
       "      <td>-0.468878</td>\n",
       "      <td>0.222251</td>\n",
       "      <td>-0.966948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.035173</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.428007</td>\n",
       "      <td>0.096745</td>\n",
       "      <td>-0.023040</td>\n",
       "      <td>0.286263</td>\n",
       "      <td>0.097310</td>\n",
       "      <td>0.483163</td>\n",
       "      <td>0.165577</td>\n",
       "      <td>-0.395518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360916</td>\n",
       "      <td>-0.705914</td>\n",
       "      <td>-0.395633</td>\n",
       "      <td>-0.212922</td>\n",
       "      <td>-0.238723</td>\n",
       "      <td>-0.073164</td>\n",
       "      <td>-0.394605</td>\n",
       "      <td>-0.525837</td>\n",
       "      <td>0.421510</td>\n",
       "      <td>-1.341952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.304661</td>\n",
       "      <td>0.061852</td>\n",
       "      <td>-0.006689</td>\n",
       "      <td>-0.005078</td>\n",
       "      <td>0.230894</td>\n",
       "      <td>0.683307</td>\n",
       "      <td>0.199798</td>\n",
       "      <td>-0.563420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.740430</td>\n",
       "      <td>-0.094195</td>\n",
       "      <td>0.304008</td>\n",
       "      <td>-0.322331</td>\n",
       "      <td>-0.137408</td>\n",
       "      <td>-0.506133</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>-0.645597</td>\n",
       "      <td>0.078471</td>\n",
       "      <td>-0.740335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.027171</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.255354</td>\n",
       "      <td>-0.022251</td>\n",
       "      <td>-0.014154</td>\n",
       "      <td>0.433372</td>\n",
       "      <td>-0.531501</td>\n",
       "      <td>0.797186</td>\n",
       "      <td>0.047269</td>\n",
       "      <td>-0.107525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361203</td>\n",
       "      <td>-0.421748</td>\n",
       "      <td>-0.127547</td>\n",
       "      <td>-0.190536</td>\n",
       "      <td>0.202091</td>\n",
       "      <td>-0.325166</td>\n",
       "      <td>-0.125970</td>\n",
       "      <td>-0.315187</td>\n",
       "      <td>0.260138</td>\n",
       "      <td>-0.701295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.228566</td>\n",
       "      <td>0.051029</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>-0.061308</td>\n",
       "      <td>0.145780</td>\n",
       "      <td>0.571819</td>\n",
       "      <td>-0.002239</td>\n",
       "      <td>-0.011296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436142</td>\n",
       "      <td>-0.057485</td>\n",
       "      <td>-0.279101</td>\n",
       "      <td>-0.209773</td>\n",
       "      <td>-0.179985</td>\n",
       "      <td>0.191691</td>\n",
       "      <td>-0.051150</td>\n",
       "      <td>-0.426018</td>\n",
       "      <td>0.463899</td>\n",
       "      <td>-0.941067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.023847</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.349891</td>\n",
       "      <td>0.186136</td>\n",
       "      <td>-0.025922</td>\n",
       "      <td>0.187013</td>\n",
       "      <td>0.251899</td>\n",
       "      <td>0.505776</td>\n",
       "      <td>0.284798</td>\n",
       "      <td>-0.256896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334975</td>\n",
       "      <td>-0.317052</td>\n",
       "      <td>-0.078558</td>\n",
       "      <td>-0.424601</td>\n",
       "      <td>-0.269827</td>\n",
       "      <td>0.044225</td>\n",
       "      <td>-0.170056</td>\n",
       "      <td>-0.558705</td>\n",
       "      <td>0.095313</td>\n",
       "      <td>-0.824074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.030401</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.327205</td>\n",
       "      <td>-0.197164</td>\n",
       "      <td>-0.023134</td>\n",
       "      <td>-0.181902</td>\n",
       "      <td>0.143999</td>\n",
       "      <td>0.504624</td>\n",
       "      <td>0.378042</td>\n",
       "      <td>0.023285</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251104</td>\n",
       "      <td>-0.075787</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.052577</td>\n",
       "      <td>-0.079473</td>\n",
       "      <td>-0.085636</td>\n",
       "      <td>-0.056602</td>\n",
       "      <td>-0.177495</td>\n",
       "      <td>0.022326</td>\n",
       "      <td>-0.256424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.046466</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.349854</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>-0.021736</td>\n",
       "      <td>0.119687</td>\n",
       "      <td>-0.213816</td>\n",
       "      <td>0.744887</td>\n",
       "      <td>0.291134</td>\n",
       "      <td>-0.139233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242990</td>\n",
       "      <td>-0.457849</td>\n",
       "      <td>-0.172432</td>\n",
       "      <td>-0.160867</td>\n",
       "      <td>-0.023765</td>\n",
       "      <td>-0.235037</td>\n",
       "      <td>-0.269953</td>\n",
       "      <td>-0.322147</td>\n",
       "      <td>-0.221361</td>\n",
       "      <td>-0.370739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.024402</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.263051</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>-0.014972</td>\n",
       "      <td>-0.278829</td>\n",
       "      <td>-0.114340</td>\n",
       "      <td>0.541088</td>\n",
       "      <td>0.210295</td>\n",
       "      <td>-0.039770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537113</td>\n",
       "      <td>0.158330</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>-0.043481</td>\n",
       "      <td>-0.159065</td>\n",
       "      <td>0.019331</td>\n",
       "      <td>0.065101</td>\n",
       "      <td>-0.239220</td>\n",
       "      <td>-0.024870</td>\n",
       "      <td>-0.149249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.039878</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.176271</td>\n",
       "      <td>-0.088139</td>\n",
       "      <td>-0.014517</td>\n",
       "      <td>0.342367</td>\n",
       "      <td>-0.057469</td>\n",
       "      <td>0.688618</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.034284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.347324</td>\n",
       "      <td>-0.015115</td>\n",
       "      <td>-0.130710</td>\n",
       "      <td>-0.115479</td>\n",
       "      <td>-0.043809</td>\n",
       "      <td>-0.209641</td>\n",
       "      <td>-0.125613</td>\n",
       "      <td>-0.374027</td>\n",
       "      <td>0.331869</td>\n",
       "      <td>-0.831508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.024712</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.316677</td>\n",
       "      <td>0.181099</td>\n",
       "      <td>-0.019837</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0.322458</td>\n",
       "      <td>0.459295</td>\n",
       "      <td>0.151927</td>\n",
       "      <td>-0.284138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617003</td>\n",
       "      <td>-0.431648</td>\n",
       "      <td>0.028006</td>\n",
       "      <td>-0.653500</td>\n",
       "      <td>-0.032795</td>\n",
       "      <td>-0.084000</td>\n",
       "      <td>-0.207411</td>\n",
       "      <td>-0.534878</td>\n",
       "      <td>0.140392</td>\n",
       "      <td>-0.882681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.320846</td>\n",
       "      <td>-0.024661</td>\n",
       "      <td>-0.018497</td>\n",
       "      <td>0.227177</td>\n",
       "      <td>0.111905</td>\n",
       "      <td>0.668842</td>\n",
       "      <td>0.219711</td>\n",
       "      <td>-0.385683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505775</td>\n",
       "      <td>-0.492188</td>\n",
       "      <td>-0.386539</td>\n",
       "      <td>0.152053</td>\n",
       "      <td>-0.073069</td>\n",
       "      <td>-0.388989</td>\n",
       "      <td>-0.319787</td>\n",
       "      <td>-0.376756</td>\n",
       "      <td>0.251857</td>\n",
       "      <td>-0.948401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.029153</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.295577</td>\n",
       "      <td>-0.092124</td>\n",
       "      <td>-0.021618</td>\n",
       "      <td>0.335415</td>\n",
       "      <td>0.361117</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.328103</td>\n",
       "      <td>-0.968283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270342</td>\n",
       "      <td>-0.473352</td>\n",
       "      <td>-0.355866</td>\n",
       "      <td>0.319675</td>\n",
       "      <td>-0.045321</td>\n",
       "      <td>-0.511743</td>\n",
       "      <td>-0.219231</td>\n",
       "      <td>-0.374024</td>\n",
       "      <td>0.188612</td>\n",
       "      <td>-0.781868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.026406</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.229859</td>\n",
       "      <td>-0.031811</td>\n",
       "      <td>-0.010559</td>\n",
       "      <td>0.017854</td>\n",
       "      <td>-0.057890</td>\n",
       "      <td>0.628847</td>\n",
       "      <td>0.274668</td>\n",
       "      <td>-0.013671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368939</td>\n",
       "      <td>0.107443</td>\n",
       "      <td>-0.096291</td>\n",
       "      <td>-0.136537</td>\n",
       "      <td>-0.011807</td>\n",
       "      <td>-0.077603</td>\n",
       "      <td>0.102184</td>\n",
       "      <td>-0.424421</td>\n",
       "      <td>0.075267</td>\n",
       "      <td>-0.397504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.037830</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.352423</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>-0.001450</td>\n",
       "      <td>0.134182</td>\n",
       "      <td>0.426241</td>\n",
       "      <td>0.765964</td>\n",
       "      <td>-0.289690</td>\n",
       "      <td>-0.243405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.623311</td>\n",
       "      <td>-0.034536</td>\n",
       "      <td>-0.550382</td>\n",
       "      <td>0.060814</td>\n",
       "      <td>-0.416825</td>\n",
       "      <td>0.115101</td>\n",
       "      <td>-0.444247</td>\n",
       "      <td>-0.347045</td>\n",
       "      <td>0.282037</td>\n",
       "      <td>-1.073328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.031585</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.393741</td>\n",
       "      <td>-0.048107</td>\n",
       "      <td>-0.018712</td>\n",
       "      <td>-0.104769</td>\n",
       "      <td>-0.153472</td>\n",
       "      <td>0.981583</td>\n",
       "      <td>-0.045457</td>\n",
       "      <td>-0.313492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348805</td>\n",
       "      <td>-0.190285</td>\n",
       "      <td>0.085540</td>\n",
       "      <td>-0.092371</td>\n",
       "      <td>-0.099965</td>\n",
       "      <td>-0.298188</td>\n",
       "      <td>-0.005833</td>\n",
       "      <td>-0.399152</td>\n",
       "      <td>0.281680</td>\n",
       "      <td>-0.686665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.030127</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.325510</td>\n",
       "      <td>-0.135717</td>\n",
       "      <td>-0.027886</td>\n",
       "      <td>-0.202455</td>\n",
       "      <td>0.133497</td>\n",
       "      <td>0.669081</td>\n",
       "      <td>0.318559</td>\n",
       "      <td>-0.159337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256875</td>\n",
       "      <td>-0.226486</td>\n",
       "      <td>-0.040209</td>\n",
       "      <td>0.035445</td>\n",
       "      <td>-0.045194</td>\n",
       "      <td>-0.271337</td>\n",
       "      <td>-0.060120</td>\n",
       "      <td>-0.261175</td>\n",
       "      <td>0.238443</td>\n",
       "      <td>-0.559738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.034617</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.306352</td>\n",
       "      <td>0.118191</td>\n",
       "      <td>-0.023920</td>\n",
       "      <td>0.111875</td>\n",
       "      <td>-0.072982</td>\n",
       "      <td>0.772603</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>-0.305443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259091</td>\n",
       "      <td>-0.080996</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>-0.149026</td>\n",
       "      <td>-0.328078</td>\n",
       "      <td>-0.017502</td>\n",
       "      <td>-0.075898</td>\n",
       "      <td>-0.417186</td>\n",
       "      <td>0.105393</td>\n",
       "      <td>-0.598477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.020056</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.305795</td>\n",
       "      <td>-0.119137</td>\n",
       "      <td>-0.030694</td>\n",
       "      <td>-0.235618</td>\n",
       "      <td>0.552337</td>\n",
       "      <td>0.549110</td>\n",
       "      <td>0.123772</td>\n",
       "      <td>-0.336995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234632</td>\n",
       "      <td>-0.509176</td>\n",
       "      <td>-0.669683</td>\n",
       "      <td>-0.129642</td>\n",
       "      <td>-0.082144</td>\n",
       "      <td>0.270910</td>\n",
       "      <td>0.058696</td>\n",
       "      <td>-0.669253</td>\n",
       "      <td>0.303385</td>\n",
       "      <td>-0.913942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.031578</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.394381</td>\n",
       "      <td>-0.039488</td>\n",
       "      <td>-0.017706</td>\n",
       "      <td>0.201103</td>\n",
       "      <td>-0.058234</td>\n",
       "      <td>0.523385</td>\n",
       "      <td>0.091043</td>\n",
       "      <td>-0.127277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271885</td>\n",
       "      <td>-0.855397</td>\n",
       "      <td>-0.115396</td>\n",
       "      <td>-0.040741</td>\n",
       "      <td>-0.205330</td>\n",
       "      <td>-0.353435</td>\n",
       "      <td>-0.264067</td>\n",
       "      <td>-0.450835</td>\n",
       "      <td>0.474791</td>\n",
       "      <td>-1.189693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.036729</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.342181</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>-0.032518</td>\n",
       "      <td>0.419007</td>\n",
       "      <td>0.152510</td>\n",
       "      <td>0.626790</td>\n",
       "      <td>0.277112</td>\n",
       "      <td>-0.728176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398492</td>\n",
       "      <td>-0.626017</td>\n",
       "      <td>-0.045108</td>\n",
       "      <td>-0.319314</td>\n",
       "      <td>-0.122679</td>\n",
       "      <td>-0.214644</td>\n",
       "      <td>-0.279868</td>\n",
       "      <td>-0.421877</td>\n",
       "      <td>0.355466</td>\n",
       "      <td>-1.057211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.039722</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.178967</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>-0.022237</td>\n",
       "      <td>0.243974</td>\n",
       "      <td>0.508167</td>\n",
       "      <td>0.654990</td>\n",
       "      <td>0.395521</td>\n",
       "      <td>-0.458853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.348946</td>\n",
       "      <td>-0.508666</td>\n",
       "      <td>-0.432208</td>\n",
       "      <td>-0.160817</td>\n",
       "      <td>-0.072521</td>\n",
       "      <td>0.022581</td>\n",
       "      <td>-0.143057</td>\n",
       "      <td>-0.499908</td>\n",
       "      <td>0.254924</td>\n",
       "      <td>-0.897889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.033545</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.364578</td>\n",
       "      <td>0.056726</td>\n",
       "      <td>-0.018577</td>\n",
       "      <td>0.312952</td>\n",
       "      <td>-0.396133</td>\n",
       "      <td>0.663678</td>\n",
       "      <td>0.458670</td>\n",
       "      <td>-0.316294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329741</td>\n",
       "      <td>-0.295741</td>\n",
       "      <td>-0.170729</td>\n",
       "      <td>-0.095566</td>\n",
       "      <td>-0.123651</td>\n",
       "      <td>-0.062025</td>\n",
       "      <td>-0.186147</td>\n",
       "      <td>-0.265824</td>\n",
       "      <td>-0.005636</td>\n",
       "      <td>-0.446335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.022087</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.217554</td>\n",
       "      <td>-0.029203</td>\n",
       "      <td>0.541886</td>\n",
       "      <td>-0.072518</td>\n",
       "      <td>0.539240</td>\n",
       "      <td>0.191292</td>\n",
       "      <td>-0.444512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038383</td>\n",
       "      <td>-0.629015</td>\n",
       "      <td>-0.305369</td>\n",
       "      <td>-0.107677</td>\n",
       "      <td>-0.034464</td>\n",
       "      <td>-0.168259</td>\n",
       "      <td>-0.105421</td>\n",
       "      <td>-0.510348</td>\n",
       "      <td>-0.166452</td>\n",
       "      <td>-0.449317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.367226</td>\n",
       "      <td>-0.044708</td>\n",
       "      <td>-0.026028</td>\n",
       "      <td>0.577617</td>\n",
       "      <td>-0.037018</td>\n",
       "      <td>0.261197</td>\n",
       "      <td>0.030637</td>\n",
       "      <td>0.209306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.353728</td>\n",
       "      <td>-0.812162</td>\n",
       "      <td>-0.581130</td>\n",
       "      <td>-0.050014</td>\n",
       "      <td>0.088620</td>\n",
       "      <td>-0.281399</td>\n",
       "      <td>-0.257853</td>\n",
       "      <td>-0.566070</td>\n",
       "      <td>0.144967</td>\n",
       "      <td>-0.968890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.435789</td>\n",
       "      <td>-0.195501</td>\n",
       "      <td>-0.021946</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.252997</td>\n",
       "      <td>0.952391</td>\n",
       "      <td>0.130792</td>\n",
       "      <td>-0.581239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393496</td>\n",
       "      <td>-0.302486</td>\n",
       "      <td>0.112044</td>\n",
       "      <td>-0.268695</td>\n",
       "      <td>-0.172738</td>\n",
       "      <td>-0.281136</td>\n",
       "      <td>-0.298529</td>\n",
       "      <td>-0.311995</td>\n",
       "      <td>0.658281</td>\n",
       "      <td>-1.268805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.049678</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.316922</td>\n",
       "      <td>0.043169</td>\n",
       "      <td>-0.023076</td>\n",
       "      <td>0.544628</td>\n",
       "      <td>0.021422</td>\n",
       "      <td>0.915477</td>\n",
       "      <td>0.055965</td>\n",
       "      <td>-0.795593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.265662</td>\n",
       "      <td>-0.681079</td>\n",
       "      <td>-0.343424</td>\n",
       "      <td>-0.304442</td>\n",
       "      <td>-0.282270</td>\n",
       "      <td>0.070631</td>\n",
       "      <td>-0.112960</td>\n",
       "      <td>-0.746545</td>\n",
       "      <td>0.250196</td>\n",
       "      <td>-1.109702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.016703</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.437332</td>\n",
       "      <td>-0.069509</td>\n",
       "      <td>-0.024343</td>\n",
       "      <td>0.147510</td>\n",
       "      <td>0.148580</td>\n",
       "      <td>0.510310</td>\n",
       "      <td>0.497946</td>\n",
       "      <td>-0.178157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.322135</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>-0.159964</td>\n",
       "      <td>-0.315567</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>0.090081</td>\n",
       "      <td>-0.119868</td>\n",
       "      <td>-0.256466</td>\n",
       "      <td>-0.037281</td>\n",
       "      <td>-0.339054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.037663</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.282879</td>\n",
       "      <td>-0.034484</td>\n",
       "      <td>-0.001951</td>\n",
       "      <td>0.233602</td>\n",
       "      <td>-0.366343</td>\n",
       "      <td>0.442451</td>\n",
       "      <td>-0.106750</td>\n",
       "      <td>-0.193125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.408062</td>\n",
       "      <td>-0.752527</td>\n",
       "      <td>-0.385266</td>\n",
       "      <td>-0.140133</td>\n",
       "      <td>-0.119936</td>\n",
       "      <td>-0.087948</td>\n",
       "      <td>-0.215907</td>\n",
       "      <td>-0.517375</td>\n",
       "      <td>0.152033</td>\n",
       "      <td>-0.885315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.034112</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.412058</td>\n",
       "      <td>-0.011817</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>0.405230</td>\n",
       "      <td>0.192549</td>\n",
       "      <td>0.684520</td>\n",
       "      <td>0.463499</td>\n",
       "      <td>-0.998743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.732592</td>\n",
       "      <td>-0.206576</td>\n",
       "      <td>-0.137135</td>\n",
       "      <td>-0.581076</td>\n",
       "      <td>-0.104350</td>\n",
       "      <td>-0.135146</td>\n",
       "      <td>-0.286507</td>\n",
       "      <td>-0.671200</td>\n",
       "      <td>0.107022</td>\n",
       "      <td>-1.064729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.027134</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.417336</td>\n",
       "      <td>-0.069021</td>\n",
       "      <td>-0.008670</td>\n",
       "      <td>0.135390</td>\n",
       "      <td>0.218093</td>\n",
       "      <td>0.628178</td>\n",
       "      <td>0.048179</td>\n",
       "      <td>-0.211806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.507520</td>\n",
       "      <td>-0.569411</td>\n",
       "      <td>-0.250868</td>\n",
       "      <td>-0.053073</td>\n",
       "      <td>-0.272443</td>\n",
       "      <td>-0.267636</td>\n",
       "      <td>-0.350094</td>\n",
       "      <td>-0.493925</td>\n",
       "      <td>0.365870</td>\n",
       "      <td>-1.209889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.020988</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.384113</td>\n",
       "      <td>0.045958</td>\n",
       "      <td>-0.017663</td>\n",
       "      <td>0.160133</td>\n",
       "      <td>-0.059248</td>\n",
       "      <td>0.607465</td>\n",
       "      <td>0.351906</td>\n",
       "      <td>-0.467108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203730</td>\n",
       "      <td>-0.331631</td>\n",
       "      <td>0.011361</td>\n",
       "      <td>-0.217121</td>\n",
       "      <td>0.098275</td>\n",
       "      <td>-0.390596</td>\n",
       "      <td>-0.181045</td>\n",
       "      <td>-0.317036</td>\n",
       "      <td>0.042333</td>\n",
       "      <td>-0.540414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.380912</td>\n",
       "      <td>-0.058330</td>\n",
       "      <td>-0.028727</td>\n",
       "      <td>0.414828</td>\n",
       "      <td>-0.064247</td>\n",
       "      <td>0.670514</td>\n",
       "      <td>0.355078</td>\n",
       "      <td>-0.696947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710781</td>\n",
       "      <td>-0.049933</td>\n",
       "      <td>-0.435279</td>\n",
       "      <td>0.513277</td>\n",
       "      <td>0.136112</td>\n",
       "      <td>-0.999612</td>\n",
       "      <td>-0.292207</td>\n",
       "      <td>-0.493295</td>\n",
       "      <td>0.411454</td>\n",
       "      <td>-1.196955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.244351</td>\n",
       "      <td>0.125225</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>0.197533</td>\n",
       "      <td>-0.362401</td>\n",
       "      <td>0.779667</td>\n",
       "      <td>0.191863</td>\n",
       "      <td>-0.401386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190793</td>\n",
       "      <td>-0.508803</td>\n",
       "      <td>-0.010754</td>\n",
       "      <td>-0.408557</td>\n",
       "      <td>-0.122299</td>\n",
       "      <td>0.102071</td>\n",
       "      <td>-0.036417</td>\n",
       "      <td>-0.403122</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>-0.527938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.024128</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.247352</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>-0.013227</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>0.743616</td>\n",
       "      <td>0.094052</td>\n",
       "      <td>-0.279950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403346</td>\n",
       "      <td>-0.455395</td>\n",
       "      <td>0.168846</td>\n",
       "      <td>-0.209445</td>\n",
       "      <td>-0.369472</td>\n",
       "      <td>-0.222883</td>\n",
       "      <td>-0.112512</td>\n",
       "      <td>-0.520441</td>\n",
       "      <td>0.765875</td>\n",
       "      <td>-1.398828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.018160</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.263616</td>\n",
       "      <td>0.233078</td>\n",
       "      <td>-0.015459</td>\n",
       "      <td>0.193540</td>\n",
       "      <td>-0.727106</td>\n",
       "      <td>0.575493</td>\n",
       "      <td>0.318203</td>\n",
       "      <td>-0.209440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098096</td>\n",
       "      <td>-0.275494</td>\n",
       "      <td>-0.245897</td>\n",
       "      <td>-0.108523</td>\n",
       "      <td>0.143782</td>\n",
       "      <td>-0.315154</td>\n",
       "      <td>-0.253771</td>\n",
       "      <td>-0.272020</td>\n",
       "      <td>0.097978</td>\n",
       "      <td>-0.623769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.061861</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.268711</td>\n",
       "      <td>-0.009927</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.360369</td>\n",
       "      <td>-0.033398</td>\n",
       "      <td>0.691317</td>\n",
       "      <td>-0.058617</td>\n",
       "      <td>-0.359423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.383620</td>\n",
       "      <td>-0.449389</td>\n",
       "      <td>-0.085201</td>\n",
       "      <td>-0.172546</td>\n",
       "      <td>-0.178132</td>\n",
       "      <td>-0.503529</td>\n",
       "      <td>-0.238418</td>\n",
       "      <td>-0.700990</td>\n",
       "      <td>-0.003166</td>\n",
       "      <td>-0.936242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.039923</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.414013</td>\n",
       "      <td>0.060202</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>0.342725</td>\n",
       "      <td>0.369892</td>\n",
       "      <td>0.419066</td>\n",
       "      <td>-0.189976</td>\n",
       "      <td>0.097145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.384060</td>\n",
       "      <td>-0.400040</td>\n",
       "      <td>-0.196372</td>\n",
       "      <td>-0.279959</td>\n",
       "      <td>-0.058920</td>\n",
       "      <td>-0.296137</td>\n",
       "      <td>-0.223269</td>\n",
       "      <td>-0.608120</td>\n",
       "      <td>-0.004977</td>\n",
       "      <td>-0.826411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.035155</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.299468</td>\n",
       "      <td>-0.054076</td>\n",
       "      <td>-0.018777</td>\n",
       "      <td>0.284153</td>\n",
       "      <td>0.409737</td>\n",
       "      <td>0.381149</td>\n",
       "      <td>0.100960</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131329</td>\n",
       "      <td>-1.068141</td>\n",
       "      <td>-0.282836</td>\n",
       "      <td>-0.125115</td>\n",
       "      <td>0.156501</td>\n",
       "      <td>-0.679197</td>\n",
       "      <td>-0.458771</td>\n",
       "      <td>-0.471876</td>\n",
       "      <td>0.321601</td>\n",
       "      <td>-1.252248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.019284</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.316136</td>\n",
       "      <td>-0.052332</td>\n",
       "      <td>-0.018208</td>\n",
       "      <td>0.252086</td>\n",
       "      <td>-0.223965</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.192245</td>\n",
       "      <td>-0.375880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.525494</td>\n",
       "      <td>-0.220993</td>\n",
       "      <td>-0.315802</td>\n",
       "      <td>-0.078705</td>\n",
       "      <td>-0.003338</td>\n",
       "      <td>-0.146872</td>\n",
       "      <td>-0.152859</td>\n",
       "      <td>-0.391857</td>\n",
       "      <td>0.053435</td>\n",
       "      <td>-0.598152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.033777</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.320156</td>\n",
       "      <td>-0.184571</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>0.506716</td>\n",
       "      <td>0.307189</td>\n",
       "      <td>0.197937</td>\n",
       "      <td>0.226358</td>\n",
       "      <td>0.158208</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340880</td>\n",
       "      <td>-0.801118</td>\n",
       "      <td>-0.684079</td>\n",
       "      <td>-0.088978</td>\n",
       "      <td>0.038553</td>\n",
       "      <td>-0.081157</td>\n",
       "      <td>-0.128781</td>\n",
       "      <td>-0.686879</td>\n",
       "      <td>0.599909</td>\n",
       "      <td>-1.415570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.044976</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.354031</td>\n",
       "      <td>-0.063959</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>0.457489</td>\n",
       "      <td>0.446351</td>\n",
       "      <td>0.632931</td>\n",
       "      <td>0.044880</td>\n",
       "      <td>-0.453589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542719</td>\n",
       "      <td>-0.392075</td>\n",
       "      <td>-0.264537</td>\n",
       "      <td>-0.288348</td>\n",
       "      <td>-0.219380</td>\n",
       "      <td>-0.075654</td>\n",
       "      <td>-0.287653</td>\n",
       "      <td>-0.560265</td>\n",
       "      <td>0.413189</td>\n",
       "      <td>-1.261108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.038980</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.296204</td>\n",
       "      <td>-0.038329</td>\n",
       "      <td>-0.026385</td>\n",
       "      <td>-0.003780</td>\n",
       "      <td>-0.145690</td>\n",
       "      <td>0.731389</td>\n",
       "      <td>0.454597</td>\n",
       "      <td>-0.198928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360655</td>\n",
       "      <td>-0.034698</td>\n",
       "      <td>0.070950</td>\n",
       "      <td>0.040904</td>\n",
       "      <td>-0.275645</td>\n",
       "      <td>-0.093079</td>\n",
       "      <td>-0.132201</td>\n",
       "      <td>-0.124669</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>-0.263285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.051607</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.377319</td>\n",
       "      <td>0.080186</td>\n",
       "      <td>-0.020766</td>\n",
       "      <td>0.667261</td>\n",
       "      <td>0.151171</td>\n",
       "      <td>0.488399</td>\n",
       "      <td>0.211359</td>\n",
       "      <td>-0.264376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330056</td>\n",
       "      <td>-0.611335</td>\n",
       "      <td>-0.497629</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>-0.231882</td>\n",
       "      <td>-0.156255</td>\n",
       "      <td>-0.329128</td>\n",
       "      <td>-0.554330</td>\n",
       "      <td>0.398372</td>\n",
       "      <td>-1.281830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.290982</td>\n",
       "      <td>0.105733</td>\n",
       "      <td>-0.015210</td>\n",
       "      <td>0.292880</td>\n",
       "      <td>0.735971</td>\n",
       "      <td>0.548174</td>\n",
       "      <td>0.230521</td>\n",
       "      <td>-0.687228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.345410</td>\n",
       "      <td>-0.837674</td>\n",
       "      <td>-0.486358</td>\n",
       "      <td>-0.303881</td>\n",
       "      <td>-0.435854</td>\n",
       "      <td>0.048405</td>\n",
       "      <td>-0.397612</td>\n",
       "      <td>-0.780077</td>\n",
       "      <td>0.291954</td>\n",
       "      <td>-1.469642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.021479</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>0.077368</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>-0.044708</td>\n",
       "      <td>-0.090949</td>\n",
       "      <td>0.763049</td>\n",
       "      <td>0.297199</td>\n",
       "      <td>-0.709121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453506</td>\n",
       "      <td>-0.220543</td>\n",
       "      <td>-0.257070</td>\n",
       "      <td>-0.365363</td>\n",
       "      <td>0.053081</td>\n",
       "      <td>-0.043753</td>\n",
       "      <td>-0.263005</td>\n",
       "      <td>-0.350101</td>\n",
       "      <td>-0.116764</td>\n",
       "      <td>-0.496342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.026649</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.322489</td>\n",
       "      <td>-0.083294</td>\n",
       "      <td>-0.017323</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>0.702715</td>\n",
       "      <td>0.322844</td>\n",
       "      <td>-0.294856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397339</td>\n",
       "      <td>-0.183744</td>\n",
       "      <td>-0.193532</td>\n",
       "      <td>-0.165179</td>\n",
       "      <td>-0.080549</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>-0.150031</td>\n",
       "      <td>-0.267456</td>\n",
       "      <td>-0.103950</td>\n",
       "      <td>-0.313537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.013039</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.361928</td>\n",
       "      <td>-0.037410</td>\n",
       "      <td>-0.022291</td>\n",
       "      <td>0.255429</td>\n",
       "      <td>0.303352</td>\n",
       "      <td>0.830038</td>\n",
       "      <td>0.270999</td>\n",
       "      <td>-0.680409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373267</td>\n",
       "      <td>-0.461377</td>\n",
       "      <td>-0.103661</td>\n",
       "      <td>-0.289070</td>\n",
       "      <td>-0.111690</td>\n",
       "      <td>-0.238583</td>\n",
       "      <td>-0.190496</td>\n",
       "      <td>-0.552508</td>\n",
       "      <td>0.342521</td>\n",
       "      <td>-1.085525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.035253</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.364566</td>\n",
       "      <td>-0.016575</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>0.525956</td>\n",
       "      <td>-0.491039</td>\n",
       "      <td>0.646591</td>\n",
       "      <td>0.213053</td>\n",
       "      <td>-0.125399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337553</td>\n",
       "      <td>-0.166004</td>\n",
       "      <td>-0.423804</td>\n",
       "      <td>0.110318</td>\n",
       "      <td>-0.262361</td>\n",
       "      <td>-0.017681</td>\n",
       "      <td>-0.196966</td>\n",
       "      <td>-0.396562</td>\n",
       "      <td>0.652814</td>\n",
       "      <td>-1.246343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.025747</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.412298</td>\n",
       "      <td>0.113197</td>\n",
       "      <td>-0.015965</td>\n",
       "      <td>0.252067</td>\n",
       "      <td>0.363342</td>\n",
       "      <td>0.509362</td>\n",
       "      <td>0.262335</td>\n",
       "      <td>-0.565960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592122</td>\n",
       "      <td>-0.435693</td>\n",
       "      <td>-0.090658</td>\n",
       "      <td>-0.513912</td>\n",
       "      <td>-0.257026</td>\n",
       "      <td>-0.014586</td>\n",
       "      <td>-0.169350</td>\n",
       "      <td>-0.706832</td>\n",
       "      <td>0.097677</td>\n",
       "      <td>-0.973859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.032671  0.000131  0.256003 -0.058306 -0.003321  0.286603  0.066050   \n",
       "1   0.025095  0.000163  0.445282  0.104586 -0.030611  0.590142 -0.093780   \n",
       "2   0.021471  0.000155  0.351746  0.082672 -0.016930 -0.146167  0.722969   \n",
       "3   0.050314  0.000015  0.185378  0.026086 -0.037256  0.187264  0.005782   \n",
       "4   0.024697  0.000101  0.202105 -0.122979 -0.005364  0.411503 -0.060393   \n",
       "5   0.031245  0.000163  0.410536  0.069726  0.000552  0.462986  0.522081   \n",
       "6   0.035841  0.000120  0.184033 -0.014322 -0.009471  0.088641 -0.151515   \n",
       "7   0.026395  0.000140  0.353871 -0.092337 -0.017035 -0.064316  0.182909   \n",
       "8   0.015057  0.000211  0.300439 -0.002715 -0.019450 -0.251944  0.005165   \n",
       "9   0.016759  0.000215  0.377367 -0.175569 -0.015515  0.477543  0.204756   \n",
       "10  0.036167  0.000131  0.327625 -0.092358 -0.003495  0.498002  0.084638   \n",
       "11  0.035173  0.000079  0.428007  0.096745 -0.023040  0.286263  0.097310   \n",
       "12  0.029057  0.000116  0.304661  0.061852 -0.006689 -0.005078  0.230894   \n",
       "13  0.027171  0.000134  0.255354 -0.022251 -0.014154  0.433372 -0.531501   \n",
       "14  0.022639  0.000136  0.228566  0.051029 -0.018307 -0.061308  0.145780   \n",
       "15  0.023847  0.000131  0.349891  0.186136 -0.025922  0.187013  0.251899   \n",
       "16  0.030401  0.000061  0.327205 -0.197164 -0.023134 -0.181902  0.143999   \n",
       "17  0.046466  0.000003  0.349854  0.076586 -0.021736  0.119687 -0.213816   \n",
       "18  0.024402  0.000051  0.263051  0.019875 -0.014972 -0.278829 -0.114340   \n",
       "19  0.039878  0.000082  0.176271 -0.088139 -0.014517  0.342367 -0.057469   \n",
       "20  0.024712  0.000188  0.316677  0.181099 -0.019837  0.225730  0.322458   \n",
       "21  0.027120  0.000171  0.320846 -0.024661 -0.018497  0.227177  0.111905   \n",
       "22  0.029153  0.000165  0.295577 -0.092124 -0.021618  0.335415  0.361117   \n",
       "23  0.026406  0.000094  0.229859 -0.031811 -0.010559  0.017854 -0.057890   \n",
       "24  0.037830  0.000075  0.352423  0.036681 -0.001450  0.134182  0.426241   \n",
       "25  0.031585  0.000105  0.393741 -0.048107 -0.018712 -0.104769 -0.153472   \n",
       "26  0.030127  0.000065  0.325510 -0.135717 -0.027886 -0.202455  0.133497   \n",
       "27  0.034617  0.000072  0.306352  0.118191 -0.023920  0.111875 -0.072982   \n",
       "28  0.020056  0.000163  0.305795 -0.119137 -0.030694 -0.235618  0.552337   \n",
       "29  0.031578  0.000142  0.394381 -0.039488 -0.017706  0.201103 -0.058234   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "70  0.036729  0.000146  0.342181  0.010645 -0.032518  0.419007  0.152510   \n",
       "71  0.039722  0.000047  0.178967  0.006635 -0.022237  0.243974  0.508167   \n",
       "72  0.033545  0.000052  0.364578  0.056726 -0.018577  0.312952 -0.396133   \n",
       "73  0.022087  0.000128  0.241300  0.217554 -0.029203  0.541886 -0.072518   \n",
       "74  0.027380  0.000171  0.367226 -0.044708 -0.026028  0.577617 -0.037018   \n",
       "75  0.016975  0.000163  0.435789 -0.195501 -0.021946  0.004397  0.252997   \n",
       "76  0.049678  0.000050  0.316922  0.043169 -0.023076  0.544628  0.021422   \n",
       "77  0.016703  0.000156  0.437332 -0.069509 -0.024343  0.147510  0.148580   \n",
       "78  0.037663  0.000168  0.282879 -0.034484 -0.001951  0.233602 -0.366343   \n",
       "79  0.034112  0.000145  0.412058 -0.011817 -0.011287  0.405230  0.192549   \n",
       "80  0.027134  0.000157  0.417336 -0.069021 -0.008670  0.135390  0.218093   \n",
       "81  0.020988  0.000137  0.384113  0.045958 -0.017663  0.160133 -0.059248   \n",
       "82  0.020566  0.000212  0.380912 -0.058330 -0.028727  0.414828 -0.064247   \n",
       "83  0.030119  0.000153  0.244351  0.125225 -0.018049  0.197533 -0.362401   \n",
       "84  0.024128  0.000152  0.247352  0.014302 -0.013227  0.275789  0.066467   \n",
       "85  0.018160  0.000105  0.263616  0.233078 -0.015459  0.193540 -0.727106   \n",
       "86  0.061861  0.000068  0.268711 -0.009927  0.010887  0.360369 -0.033398   \n",
       "87  0.039923  0.000161  0.414013  0.060202 -0.019534  0.342725  0.369892   \n",
       "88  0.035155  0.000110  0.299468 -0.054076 -0.018777  0.284153  0.409737   \n",
       "89  0.019284  0.000193  0.316136 -0.052332 -0.018208  0.252086 -0.223965   \n",
       "90  0.033777  0.000119  0.320156 -0.184571 -0.016607  0.506716  0.307189   \n",
       "91  0.044976  0.000125  0.354031 -0.063959 -0.007907  0.457489  0.446351   \n",
       "92  0.038980  0.000066  0.296204 -0.038329 -0.026385 -0.003780 -0.145690   \n",
       "93  0.051607  0.000005  0.377319  0.080186 -0.020766  0.667261  0.151171   \n",
       "94  0.026718  0.000086  0.290982  0.105733 -0.015210  0.292880  0.735971   \n",
       "95  0.021479  0.000130  0.368140  0.077368 -0.002310 -0.044708 -0.090949   \n",
       "96  0.026649  0.000093  0.322489 -0.083294 -0.017323  0.013615  0.038721   \n",
       "97  0.013039  0.000152  0.361928 -0.037410 -0.022291  0.255429  0.303352   \n",
       "98  0.035253  0.000054  0.364566 -0.016575 -0.034231  0.525956 -0.491039   \n",
       "99  0.025747  0.000183  0.412298  0.113197 -0.015965  0.252067  0.363342   \n",
       "\n",
       "          7         8         9   ...        51        52        53        54  \\\n",
       "0   0.874769  0.078894 -0.731736  ... -0.227157 -0.897767 -0.610010 -0.083917   \n",
       "1   0.517398 -0.019519 -0.144523  ... -0.361042 -0.463306 -0.386534  0.004146   \n",
       "2   0.647529  0.408096 -0.889756  ... -0.412127 -0.467079 -0.627033 -0.001143   \n",
       "3   0.610711  0.105193 -0.177144  ... -0.057431 -0.571172 -0.821983  0.241457   \n",
       "4   0.675709  0.136873 -0.132811  ... -0.156505 -0.310036 -0.143884  0.001543   \n",
       "5   0.745727  0.200726 -0.834184  ... -0.481257 -0.514269 -0.263155 -0.068381   \n",
       "6   0.774915  0.218633 -0.071132  ... -0.287983 -0.190941 -0.106361 -0.117559   \n",
       "7   0.706089  0.135378 -0.309628  ... -0.445184 -0.080798 -0.180830  0.089259   \n",
       "8   0.550600  0.154253 -0.095307  ... -0.364931  0.046038 -0.219722  0.310221   \n",
       "9   0.697222  0.250206 -0.785094  ... -0.533982 -0.546195 -0.168957  0.063893   \n",
       "10  0.632592  0.082388 -0.287607  ... -0.410985 -0.582586  0.255945 -0.128399   \n",
       "11  0.483163  0.165577 -0.395518  ... -0.360916 -0.705914 -0.395633 -0.212922   \n",
       "12  0.683307  0.199798 -0.563420  ... -0.740430 -0.094195  0.304008 -0.322331   \n",
       "13  0.797186  0.047269 -0.107525  ... -0.361203 -0.421748 -0.127547 -0.190536   \n",
       "14  0.571819 -0.002239 -0.011296  ... -0.436142 -0.057485 -0.279101 -0.209773   \n",
       "15  0.505776  0.284798 -0.256896  ... -0.334975 -0.317052 -0.078558 -0.424601   \n",
       "16  0.504624  0.378042  0.023285  ... -0.251104 -0.075787 -0.016411 -0.052577   \n",
       "17  0.744887  0.291134 -0.139233  ... -0.242990 -0.457849 -0.172432 -0.160867   \n",
       "18  0.541088  0.210295 -0.039770  ... -0.537113  0.158330  0.009096 -0.043481   \n",
       "19  0.688618  0.023529  0.034284  ... -0.347324 -0.015115 -0.130710 -0.115479   \n",
       "20  0.459295  0.151927 -0.284138  ... -0.617003 -0.431648  0.028006 -0.653500   \n",
       "21  0.668842  0.219711 -0.385683  ... -0.505775 -0.492188 -0.386539  0.152053   \n",
       "22  0.851700  0.328103 -0.968283  ... -0.270342 -0.473352 -0.355866  0.319675   \n",
       "23  0.628847  0.274668 -0.013671  ... -0.368939  0.107443 -0.096291 -0.136537   \n",
       "24  0.765964 -0.289690 -0.243405  ... -0.623311 -0.034536 -0.550382  0.060814   \n",
       "25  0.981583 -0.045457 -0.313492  ... -0.348805 -0.190285  0.085540 -0.092371   \n",
       "26  0.669081  0.318559 -0.159337  ... -0.256875 -0.226486 -0.040209  0.035445   \n",
       "27  0.772603  0.026128 -0.305443  ... -0.259091 -0.080996  0.001522 -0.149026   \n",
       "28  0.549110  0.123772 -0.336995  ... -0.234632 -0.509176 -0.669683 -0.129642   \n",
       "29  0.523385  0.091043 -0.127277  ... -0.271885 -0.855397 -0.115396 -0.040741   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "70  0.626790  0.277112 -0.728176  ... -0.398492 -0.626017 -0.045108 -0.319314   \n",
       "71  0.654990  0.395521 -0.458853  ... -0.348946 -0.508666 -0.432208 -0.160817   \n",
       "72  0.663678  0.458670 -0.316294  ... -0.329741 -0.295741 -0.170729 -0.095566   \n",
       "73  0.539240  0.191292 -0.444512  ... -0.038383 -0.629015 -0.305369 -0.107677   \n",
       "74  0.261197  0.030637  0.209306  ... -0.353728 -0.812162 -0.581130 -0.050014   \n",
       "75  0.952391  0.130792 -0.581239  ... -0.393496 -0.302486  0.112044 -0.268695   \n",
       "76  0.915477  0.055965 -0.795593  ... -0.265662 -0.681079 -0.343424 -0.304442   \n",
       "77  0.510310  0.497946 -0.178157  ... -0.322135  0.014669 -0.159964 -0.315567   \n",
       "78  0.442451 -0.106750 -0.193125  ... -0.408062 -0.752527 -0.385266 -0.140133   \n",
       "79  0.684520  0.463499 -0.998743  ... -0.732592 -0.206576 -0.137135 -0.581076   \n",
       "80  0.628178  0.048179 -0.211806  ... -0.507520 -0.569411 -0.250868 -0.053073   \n",
       "81  0.607465  0.351906 -0.467108  ... -0.203730 -0.331631  0.011361 -0.217121   \n",
       "82  0.670514  0.355078 -0.696947  ... -0.710781 -0.049933 -0.435279  0.513277   \n",
       "83  0.779667  0.191863 -0.401386  ... -0.190793 -0.508803 -0.010754 -0.408557   \n",
       "84  0.743616  0.094052 -0.279950  ... -0.403346 -0.455395  0.168846 -0.209445   \n",
       "85  0.575493  0.318203 -0.209440  ... -0.098096 -0.275494 -0.245897 -0.108523   \n",
       "86  0.691317 -0.058617 -0.359423  ... -0.383620 -0.449389 -0.085201 -0.172546   \n",
       "87  0.419066 -0.189976  0.097145  ... -0.384060 -0.400040 -0.196372 -0.279959   \n",
       "88  0.381149  0.100960  0.122300  ... -0.131329 -1.068141 -0.282836 -0.125115   \n",
       "89  0.535500  0.192245 -0.375880  ... -0.525494 -0.220993 -0.315802 -0.078705   \n",
       "90  0.197937  0.226358  0.158208  ... -0.340880 -0.801118 -0.684079 -0.088978   \n",
       "91  0.632931  0.044880 -0.453589  ... -0.542719 -0.392075 -0.264537 -0.288348   \n",
       "92  0.731389  0.454597 -0.198928  ... -0.360655 -0.034698  0.070950  0.040904   \n",
       "93  0.488399  0.211359 -0.264376  ... -0.330056 -0.611335 -0.497629  0.002308   \n",
       "94  0.548174  0.230521 -0.687228  ... -0.345410 -0.837674 -0.486358 -0.303881   \n",
       "95  0.763049  0.297199 -0.709121  ... -0.453506 -0.220543 -0.257070 -0.365363   \n",
       "96  0.702715  0.322844 -0.294856  ... -0.397339 -0.183744 -0.193532 -0.165179   \n",
       "97  0.830038  0.270999 -0.680409  ... -0.373267 -0.461377 -0.103661 -0.289070   \n",
       "98  0.646591  0.213053 -0.125399  ... -0.337553 -0.166004 -0.423804  0.110318   \n",
       "99  0.509362  0.262335 -0.565960  ... -0.592122 -0.435693 -0.090658 -0.513912   \n",
       "\n",
       "          55        56        57        58        59        60  \n",
       "0  -0.366080  0.228062 -0.096377 -0.735567  0.414970 -1.246915  \n",
       "1  -0.244736 -0.085657 -0.129780 -0.583002 -0.105507 -0.607274  \n",
       "2  -0.158485 -0.007982 -0.201590 -0.593053  0.148284 -0.942927  \n",
       "3   0.091958 -0.080729 -0.111169 -0.458128  0.345634 -0.914931  \n",
       "4  -0.094179 -0.230241 -0.154570 -0.312192  0.163843 -0.630605  \n",
       "5  -0.377578 -0.311870 -0.351235 -0.669750 -0.077694 -0.943291  \n",
       "6  -0.040216 -0.177651 -0.114126 -0.327663 -0.160293 -0.281495  \n",
       "7   0.036199 -0.372982 -0.186342 -0.242012  0.055065 -0.483419  \n",
       "8  -0.055993 -0.431610 -0.071361 -0.325745  0.096347 -0.493453  \n",
       "9  -0.169763 -0.570421 -0.325795 -0.519452  0.587066 -1.432313  \n",
       "10 -0.345089 -0.527155 -0.275819 -0.468878  0.222251 -0.966948  \n",
       "11 -0.238723 -0.073164 -0.394605 -0.525837  0.421510 -1.341952  \n",
       "12 -0.137408 -0.506133 -0.016267 -0.645597  0.078471 -0.740335  \n",
       "13  0.202091 -0.325166 -0.125970 -0.315187  0.260138 -0.701295  \n",
       "14 -0.179985  0.191691 -0.051150 -0.426018  0.463899 -0.941067  \n",
       "15 -0.269827  0.044225 -0.170056 -0.558705  0.095313 -0.824074  \n",
       "16 -0.079473 -0.085636 -0.056602 -0.177495  0.022326 -0.256424  \n",
       "17 -0.023765 -0.235037 -0.269953 -0.322147 -0.221361 -0.370739  \n",
       "18 -0.159065  0.019331  0.065101 -0.239220 -0.024870 -0.149249  \n",
       "19 -0.043809 -0.209641 -0.125613 -0.374027  0.331869 -0.831508  \n",
       "20 -0.032795 -0.084000 -0.207411 -0.534878  0.140392 -0.882681  \n",
       "21 -0.073069 -0.388989 -0.319787 -0.376756  0.251857 -0.948401  \n",
       "22 -0.045321 -0.511743 -0.219231 -0.374024  0.188612 -0.781868  \n",
       "23 -0.011807 -0.077603  0.102184 -0.424421  0.075267 -0.397504  \n",
       "24 -0.416825  0.115101 -0.444247 -0.347045  0.282037 -1.073328  \n",
       "25 -0.099965 -0.298188 -0.005833 -0.399152  0.281680 -0.686665  \n",
       "26 -0.045194 -0.271337 -0.060120 -0.261175  0.238443 -0.559738  \n",
       "27 -0.328078 -0.017502 -0.075898 -0.417186  0.105393 -0.598477  \n",
       "28 -0.082144  0.270910  0.058696 -0.669253  0.303385 -0.913942  \n",
       "29 -0.205330 -0.353435 -0.264067 -0.450835  0.474791 -1.189693  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "70 -0.122679 -0.214644 -0.279868 -0.421877  0.355466 -1.057211  \n",
       "71 -0.072521  0.022581 -0.143057 -0.499908  0.254924 -0.897889  \n",
       "72 -0.123651 -0.062025 -0.186147 -0.265824 -0.005636 -0.446335  \n",
       "73 -0.034464 -0.168259 -0.105421 -0.510348 -0.166452 -0.449317  \n",
       "74  0.088620 -0.281399 -0.257853 -0.566070  0.144967 -0.968890  \n",
       "75 -0.172738 -0.281136 -0.298529 -0.311995  0.658281 -1.268805  \n",
       "76 -0.282270  0.070631 -0.112960 -0.746545  0.250196 -1.109702  \n",
       "77  0.009115  0.090081 -0.119868 -0.256466 -0.037281 -0.339054  \n",
       "78 -0.119936 -0.087948 -0.215907 -0.517375  0.152033 -0.885315  \n",
       "79 -0.104350 -0.135146 -0.286507 -0.671200  0.107022 -1.064729  \n",
       "80 -0.272443 -0.267636 -0.350094 -0.493925  0.365870 -1.209889  \n",
       "81  0.098275 -0.390596 -0.181045 -0.317036  0.042333 -0.540414  \n",
       "82  0.136112 -0.999612 -0.292207 -0.493295  0.411454 -1.196955  \n",
       "83 -0.122299  0.102071 -0.036417 -0.403122  0.088400 -0.527938  \n",
       "84 -0.369472 -0.222883 -0.112512 -0.520441  0.765875 -1.398828  \n",
       "85  0.143782 -0.315154 -0.253771 -0.272020  0.097978 -0.623769  \n",
       "86 -0.178132 -0.503529 -0.238418 -0.700990 -0.003166 -0.936242  \n",
       "87 -0.058920 -0.296137 -0.223269 -0.608120 -0.004977 -0.826411  \n",
       "88  0.156501 -0.679197 -0.458771 -0.471876  0.321601 -1.252248  \n",
       "89 -0.003338 -0.146872 -0.152859 -0.391857  0.053435 -0.598152  \n",
       "90  0.038553 -0.081157 -0.128781 -0.686879  0.599909 -1.415570  \n",
       "91 -0.219380 -0.075654 -0.287653 -0.560265  0.413189 -1.261108  \n",
       "92 -0.275645 -0.093079 -0.132201 -0.124669  0.006415 -0.263285  \n",
       "93 -0.231882 -0.156255 -0.329128 -0.554330  0.398372 -1.281830  \n",
       "94 -0.435854  0.048405 -0.397612 -0.780077  0.291954 -1.469642  \n",
       "95  0.053081 -0.043753 -0.263005 -0.350101 -0.116764 -0.496342  \n",
       "96 -0.080549  0.021774 -0.150031 -0.267456 -0.103950 -0.313537  \n",
       "97 -0.111690 -0.238583 -0.190496 -0.552508  0.342521 -1.085525  \n",
       "98 -0.262361 -0.017681 -0.196966 -0.396562  0.652814 -1.246343  \n",
       "99 -0.257026 -0.014586 -0.169350 -0.706832  0.097677 -0.973859  \n",
       "\n",
       "[100 rows x 61 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramatros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varianza de los estimadores\n",
    "Como se puede observar las varianzas de los parametros de los 100 modelos son cercanas a cero, lo que sugiere estabilidad en el modelo  y que los parametros no dependeran de los datos con que ha sido entrenado y a pesar de que utilice una fuente de entrenamiento diferente los parametros son muestran estabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8.924147e-05\n",
       "1     2.026380e-09\n",
       "2     5.659725e-03\n",
       "3     8.286682e-03\n",
       "4     8.976482e-05\n",
       "5     5.196595e-02\n",
       "6     7.852071e-02\n",
       "7     2.222371e-02\n",
       "8     2.067581e-02\n",
       "9     7.388158e-02\n",
       "10    2.395845e-02\n",
       "11    6.903227e-02\n",
       "12    8.890777e-02\n",
       "13    3.074476e-02\n",
       "14    1.235242e-01\n",
       "15    5.609330e-02\n",
       "16    3.217300e-02\n",
       "17    8.242234e-02\n",
       "18    6.344895e-02\n",
       "19    4.664152e-02\n",
       "20    3.343637e-02\n",
       "21    3.889619e-02\n",
       "22    7.140910e-02\n",
       "23    1.090161e-01\n",
       "24    2.437993e-02\n",
       "25    7.337504e-02\n",
       "26    2.094608e-02\n",
       "27    5.670370e-02\n",
       "28    1.106236e-01\n",
       "29    1.094580e-01\n",
       "          ...     \n",
       "31    8.808992e-02\n",
       "32    4.305099e-02\n",
       "33    2.325357e-02\n",
       "34    4.732102e-02\n",
       "35    3.159673e-02\n",
       "36    7.005981e-02\n",
       "37    2.777580e-02\n",
       "38    3.387002e-02\n",
       "39    4.618254e-02\n",
       "40    4.458333e-02\n",
       "41    6.724623e-02\n",
       "42    9.911021e-02\n",
       "43    2.627848e-02\n",
       "44    2.324100e-02\n",
       "45    2.325085e-02\n",
       "46    4.528961e-02\n",
       "47    4.486388e-02\n",
       "48    8.308154e-02\n",
       "49    2.374248e-02\n",
       "50    2.496879e-02\n",
       "51    3.015164e-02\n",
       "52    7.545990e-02\n",
       "53    5.565408e-02\n",
       "54    3.917126e-02\n",
       "55    2.336048e-02\n",
       "56    5.178913e-02\n",
       "57    1.360068e-02\n",
       "58    2.252431e-02\n",
       "59    3.557676e-02\n",
       "60    9.436196e-02\n",
       "Length: 61, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Varianzas de los 60 estimadores\n",
    "paramatros.apply( np.var, axis = 0  ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluacion de los modelos\n",
    "Hasta ahora hemos estimado los parámetros del modelo logístico. Pero no hemos examinado si la solución es satisfactoria. A continuación examinemos el desempeño de los modelos a partir de su matriz de confusión: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[215  72]\n",
      " [ 82  31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculamos la matriz de confusión para la prediccion\n",
    "cm_log = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mejorar esta visualización, añadiendo etiquetas para lo que es predicción y lo que es observado en la muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Matriz de confusión', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta verdadera')\n",
    "    plt.xlabel('Etiqueta estimada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels=['Bueno (Default=0)' ,'Malo (Default=1)']\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos resumir estos resultados, por ejemplo mediante la métrica de *exactitud* o *accuracy*, la cual mide la proporción de aciertos sobre el total de casos.\n",
    "\n",
    "Para entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float((y_tr == y_train).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.615"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float((y_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5.1\n",
    "\n",
    "Calcule una métrica de desempeño que tome en cuenta el coste de errar, donde el coste de predecir que un mal cliente es bueno es 5 veces más alto que el de confundir un buen cliente con uno malo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la función calculada se puede generar una función de costo a partir de la matriz de confusión. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35755813953488375"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculamos la matriz de confusión para la prediccion\n",
    "cm_log = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#Matriz de confusión con Penalización\n",
    "n_2=cm_log [0,0] + cm_log [0,1]*5 + cm_log [1,0] + cm_log [1,1]\n",
    "\n",
    "Acc_2=(cm_log [0,0]  + cm_log [1,1])/n_2\n",
    "Acc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se ve como la nueva medida de desempeño que penaliza los falsos positivos pasa a ser del 35%, casi la mitad del Accuracy anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justicia algorítmica\n",
    "\n",
    "Veamos cómo se comporta el modelo de acuerdo con el origen (extranjero o local) del cliente. Por ejemplo, fijémonos en el balance inicial de los datos de la muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_2 = credit_1.copy()\n",
    "credit_3 = credit_2.loc[credit_2['foreign_A201'] == 1]\n",
    "X3 = credit_3.iloc[:, 1:62]\n",
    "Y3 = credit_3.iloc[:, 0]\n",
    "\n",
    "credit_4 = credit_2.loc[credit_2['foreign_A201'] == 0]\n",
    "X4 = credit_4.iloc[:, 1:62]\n",
    "Y4 = credit_4.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El numero de clientes extranjeros de la muestra: 963 y los locales: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"El numero de clientes extranjeros de la muestra: \" +str(X3.shape[0]) +\" y los locales: \" +str(X4.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 5.2\n",
    "\n",
    "De los clientes locales cuantos han tenido Default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33\n",
       "1     4\n",
       "Name: Default, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y4.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vemos que ya hay un sesgo en los datos de entrenamiento. Por lo tanto, podemos esperar que esto se vea reflejado en nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_f = logT.predict(X3)\n",
    "y_pred_l = logT.predict(X4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos otra manera de visualizar la matriz de confusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extranjeros:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locales:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "print(\"Extranjeros:\")\n",
    "plot_confusion_matrix(logT, X3, Y3, normalize='all')  \n",
    "plt.show() \n",
    "\n",
    "print(\"Locales:\")\n",
    "plot_confusion_matrix(logT, X4, Y4, normalize='all')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las métricas de exactitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La exactitud para el usuario extranjero: 0.770508826583593\n",
      "La exactitud para el usuario local: 0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "print(\"La exactitud para el usuario extranjero:\", float((y_pred_f == Y3).mean()))\n",
    "print(\"La exactitud para el usuario local:\", float((y_pred_l == Y4).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5.3\n",
    "\n",
    "Proponga una modelo de clasificación que detecte clientes con un alto riesgo de Default, teniendo en cuenta el costo de clasificar a un cliente erroneamente  junto con la \"equidad algoritmica\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se instala la función fairlearn que facilita la correción de modelos que requieren equidad algoritmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/adebayoj/fairml/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairml import  audit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar mediante esta libreria se observa que la variable que mas sesgo le agrega al modelo es foreing de esta manera se elimina del modelo y adicionalmente se cambien los pesos de la clase 1 con el fin de penalizar mas los clientes en default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty='l2', C=0.01)\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la función audit_model determinamos cuales son las variables que estan generando sesgo, y como se puede observar la varaible foreing es la que tiene el mayor aporte de sesgo al modelo, se busca generar alguna manera de hacer la correción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: duration,\t Importance: 0.175\n",
      "Feature: amount,\t Importance: 0.175\n",
      "Feature: installment,\t Importance: 0.297\n",
      "Feature: residence,\t Importance: -0.322\n",
      "Feature: age,\t Importance: -0.359\n",
      "Feature: cards,\t Importance: -0.341\n",
      "Feature: liable,\t Importance: -0.319\n",
      "Feature: checkingstatus1_A11,\t Importance: 0.066\n",
      "Feature: checkingstatus1_A12,\t Importance: 0.068\n",
      "Feature: checkingstatus1_A13,\t Importance: -0.031\n",
      "Feature: checkingstatus1_A14,\t Importance: -0.143\n",
      "Feature: history_A30,\t Importance: 0.005\n",
      "Feature: history_A31,\t Importance: 0.009\n",
      "Feature: history_A32,\t Importance: -0.161\n",
      "Feature: history_A33,\t Importance: -0.027\n",
      "Feature: history_A34,\t Importance: -0.102\n",
      "Feature: purpose_A40,\t Importance: 0.084\n",
      "Feature: purpose_A41,\t Importance: -0.037\n",
      "Feature: purpose_A410,\t Importance: 0.002\n",
      "Feature: purpose_A42,\t Importance: -0.056\n",
      "Feature: purpose_A43,\t Importance: -0.085\n",
      "Feature: purpose_A44,\t Importance: 0.005\n",
      "Feature: purpose_A45,\t Importance: 0.008\n",
      "Feature: purpose_A46,\t Importance: 0.015\n",
      "Feature: purpose_A48,\t Importance: -0.003\n",
      "Feature: purpose_A49,\t Importance: 0.028\n",
      "Feature: savings_A61,\t Importance: 0.191\n",
      "Feature: savings_A62,\t Importance: 0.026\n",
      "Feature: savings_A63,\t Importance: -0.025\n",
      "Feature: savings_A64,\t Importance: -0.016\n",
      "Feature: savings_A65,\t Importance: -0.065\n",
      "Feature: employ_A71,\t Importance: 0.021\n",
      "Feature: employ_A72,\t Importance: 0.039\n",
      "Feature: employ_A73,\t Importance: -0.111\n",
      "Feature: employ_A74,\t Importance: -0.054\n",
      "Feature: employ_A75,\t Importance: -0.082\n",
      "Feature: status_A91,\t Importance: 0.015\n",
      "Feature: status_A92,\t Importance: 0.096\n",
      "Feature: status_A93,\t Importance: -0.152\n",
      "Feature: status_A94,\t Importance: -0.031\n",
      "Feature: others_A101,\t Importance: -0.317\n",
      "Feature: others_A102,\t Importance: 0.008\n",
      "Feature: others_A103,\t Importance: -0.018\n",
      "Feature: property_A121,\t Importance: -0.125\n",
      "Feature: property_A122,\t Importance: 0.079\n",
      "Feature: property_A123,\t Importance: -0.098\n",
      "Feature: property_A124,\t Importance: 0.032\n",
      "Feature: otherplans_A141,\t Importance: 0.037\n",
      "Feature: otherplans_A142,\t Importance: 0.017\n",
      "Feature: otherplans_A143,\t Importance: -0.286\n",
      "Feature: housing_A151,\t Importance: 0.044\n",
      "Feature: housing_A152,\t Importance: -0.209\n",
      "Feature: housing_A153,\t Importance: 0.024\n",
      "Feature: job_A171,\t Importance: 0.007\n",
      "Feature: job_A172,\t Importance: 0.075\n",
      "Feature: job_A173,\t Importance: -0.169\n",
      "Feature: job_A174,\t Importance: 0.037\n",
      "Feature: tele_A191,\t Importance: 0.209\n",
      "Feature: tele_A192,\t Importance: -0.124\n",
      "Feature: foreign_A201,\t Importance: -0.319\n",
      "Feature: foreign_A202,\t Importance: -0.009\n"
     ]
    }
   ],
   "source": [
    "total, _ = audit_model(clf.predict, X)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante este modelo generamos una función de perdida en la cual le da  5 veces mas de peso a las observaciones de default, permitiendo asi que el modelo identifique que el equivocarse en esta categoria es mucho mas costoso que equivocarse en la categoria de no default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight=[{0: 1, 1: 5}], max_iter=1500)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression( max_iter = 1500 ,  class_weight =[{0:1, 1: 5}]  )\n",
    "\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La exactitud para el usuario extranjero: 0.7694704049844237\n",
      "La exactitud para el usuario local: 0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "y_pred_f = clf.predict(X3)\n",
    "y_pred_l = clf.predict(X4)\n",
    "print(\"La exactitud para el usuario extranjero:\", float((y_pred_f == Y3).mean()))\n",
    "print(\"La exactitud para el usuario local:\", float((y_pred_l == Y4).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se importan las librerias que permitiran añadir al modelo la etica algortimica y de esta manera mejorarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import group_summary\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight=[{0: 1, 1: 5}], max_iter=1500)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression( max_iter  = 1500,  class_weight =[{0:1, 1: 5}])\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añaden restricciones al modelo que permiten brindar mayor equidad a costo de la precisión del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair = X.foreign_A201\n",
    "constraint = DemographicParity()\n",
    "classifier = LogisticRegression( max_iter  = 1500,  class_weight =[{0:1, 1: 5}])\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X, Y, sensitive_features= fair)\n",
    "y_pred_mitigated = mitigator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62 0.07]\n",
      " [0.15 0.16]]\n",
      "[[0.76 0.14]\n",
      " [0.03 0.08]]\n"
     ]
    }
   ],
   "source": [
    "cm_log3 = confusion_matrix(Y3, mitigator.predict(X3) , normalize = \"all\")\n",
    "cm_log4 = confusion_matrix(Y4, mitigator.predict(X4), normalize = \"all\" )\n",
    "\n",
    "print(cm_log3)\n",
    "print(cm_log4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La exactitud para el usuario extranjero: 0.7819314641744548\n",
      "La exactitud para el usuario local: 0.8648648648648649\n"
     ]
    }
   ],
   "source": [
    "y_pred_f = mitigator.predict(X3)\n",
    "y_pred_l = mitigator.predict(X4)\n",
    "print(\"La exactitud para el usuario extranjero:\", float((y_pred_f == Y3).mean()))\n",
    "print(\"La exactitud para el usuario local:\", float((y_pred_l == Y4).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como se puede observar el modelo mejora en la precisión de los dos grupos, haciendo que la diferencia de precisión entre los dos no sea tan amplia pero con el costo de la precisión del modelo del usuario local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecturas avanzadas\n",
    "\n",
    "Para ler más sobre la ética algoritmica puede ver: Pessach, D., Shmueli, E. (2020) Algorithmic fairness. https://arxiv.org/abs/2001.09784 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
